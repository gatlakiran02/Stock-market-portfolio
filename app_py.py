# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sbDHTgXAjfh9dcNqff1YWmspp7hFgIKb
"""

!pip install -q streamlit yfinance gym matplotlib pyngrok pandas

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from collections import defaultdict
# import numpy as np
# import matplotlib.pyplot as plt
# import gym
# from gym import spaces
# import yfinance as yf
# import pandas as pd
# 
# # --- Custom Environment ---
# class StockTradingEnv(gym.Env):
#     def __init__(self, ticker="BTC-USD", start="2020-01-01", end="2022-01-01"):
#         super().__init__()
#         data = yf.download(ticker, start=start, end=end, progress=False)
#         self.prices = data["Close"].dropna().values
#         if len(self.prices) < 2:
#             raise ValueError("Not enough data.")
#         self.initial_balance = 1000
#         self.action_space = spaces.Discrete(3)  # Buy, Hold, Sell
#         self.observation_space = spaces.Box(low=0, high=np.inf, shape=(3,), dtype=np.float32)
# 
#     def reset(self):
#         self.balance = self.initial_balance
#         self.stock_held = 0
#         self.current_step = 0
#         return self._get_obs()
# 
#     def _get_obs(self):
#         price = float(self.prices[self.current_step])
#         return np.array([price, self.balance, self.stock_held], dtype=np.float32)
# 
#     def step(self, action):
#         price = float(self.prices[self.current_step])
#         if action == 0 and self.balance >= price:
#             self.stock_held += 1
#             self.balance -= price
#         elif action == 2 and self.stock_held > 0:
#             self.stock_held -= 1
#             self.balance += price
#         self.current_step += 1
#         done = self.current_step >= len(self.prices) - 1
#         total_assets = self.balance + self.stock_held * price
#         reward = total_assets - self.initial_balance
#         return self._get_obs(), reward, done, {}
# 
# # --- Monte Carlo Agent ---
# def monte_carlo(env, episodes=2000, gamma=0.95):
#     Q = defaultdict(lambda: np.zeros(env.action_space.n))
#     returns = defaultdict(list)
#     for ep in range(episodes):
#         state = env.reset()
#         episode = []
#         done = False
#         while not done:
#             state_key = tuple(np.round(state, 1))
#             action = np.random.choice(env.action_space.n)
#             next_state, reward, done, _ = env.step(action)
#             episode.append((state_key, action, reward))
#             state = next_state
#         G = 0
#         visited = set()
#         for t in reversed(range(len(episode))):
#             s, a, r = episode[t]
#             G = gamma * G + r
#             if (s, a) not in visited:
#                 returns[(s, a)].append(G)
#                 Q[s][a] = np.mean(returns[(s, a)])
#                 visited.add((s, a))
#     return Q
# 
# def test_policy(env, Q):
#     state = env.reset()
#     done = False
#     rewards = []
#     steps = []
#     while not done:
#         state_key = tuple(np.round(state, 1))
#         action = np.argmax(Q[state_key]) if state_key in Q else np.random.choice(env.action_space.n)
#         state, reward, done, _ = env.step(action)
#         rewards.append(reward)
#         steps.append(env.current_step)
#     return steps, rewards
# 
# # --- Streamlit UI ---
# st.set_page_config(page_title="MC RL Trader", layout="centered")
# st.title("ðŸ“ˆ Monte Carlo RL for Stock Trading")
# 
# ticker = st.sidebar.text_input("Stock Ticker", value="AAPL")
# start_date = st.sidebar.date_input("Start Date", value=pd.to_datetime("2020-01-01"))
# end_date = st.sidebar.date_input("End Date", value=pd.to_datetime("2022-01-01"))
# episodes = st.sidebar.slider("Training Episodes", 500, 5000, 2000, step=500)
# 
# if st.button("Train and Test"):
#     with st.spinner("Training RL agent..."):
#         try:
#             env = StockTradingEnv(ticker=ticker, start=str(start_date), end=str(end_date))
#             Q = monte_carlo(env, episodes=episodes)
#             steps, rewards = test_policy(env, Q)
# 
#             fig, ax = plt.subplots()
#             ax.plot(steps, rewards, label="Portfolio Reward")
#             ax.set_xlabel("Steps")
#             ax.set_ylabel("Return")
#             ax.set_title("Test Performance")
#             ax.grid(True)
#             st.pyplot(fig)
#         except Exception as e:
#             st.error(f"âŒ {e}")
#

!pip install pyngrok

from pyngrok import ngrok, conf

# Set your ngrok authtoken here (replace the placeholder with your token)
conf.get_default().auth_token = "2vysAqNOTgg8rItXN7aghLRDOQC_5x7pt2u5EEHhd5yERbDKa"

from pyngrok import ngrok
!pip install pyngrok streamlit
# Kill any existing Streamlit process
!pkill streamlit
!pkill -f ngrok && pkill -f streamlit
# Start Streamlit in the background
!streamlit run app.py &>/dev/null &

# Establish a tunnel and expose port 8501 (Streamlit default port)
public_url = ngrok.connect(8501)
print("ðŸš€ Streamlit is live at:", public_url)